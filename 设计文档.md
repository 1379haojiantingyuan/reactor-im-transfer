# Unix 高级编程课程设计：基于 Reactor 模式的高并发文件传输与即时通讯系统

## 1. 项目概述 (Project Overview)

### 1.1 项目背景

在现代网络服务开发中，高并发处理能力和高效的大文件传输是衡量服务器性能的核心指标。传统的“每连接每线程”（Thread-per-connection）模型在面对海量并发连接时，由于上下文切换开销巨大，已无法满足高性能需求。

### 1.2 设计目标

本项目旨在在 Linux 环境下，使用 C/C++ 语言设计并实现一个**高并发、低延迟、高可靠**的即时通讯与文件传输系统。系统采用 **Reactor 事件驱动模型**，结合 **Epoll IO多路复用** 和 **线程池** 技术，实现业务处理与网络IO的分离；利用 **Zero-Copy (Sendfile)** 技术实现GB级大文件的高速传输。

### 1.3 核心功能

- **多用户即时通讯**：支持用户注册、登录、群聊、私聊。
- **高速文件下载**：支持客户端请求下载服务器端文件，采用内核态零拷贝技术。
- **系统监控**：服务端实时显示在线人数、吞吐量等状态。
- **可视化客户端**：基于 ncurses 库的终端图形界面 (TUI)。

## 2. 系统架构设计 (System Architecture)

### 2.1 总体架构：Reactor + Thread Pool 模型

本系统摒弃简单的阻塞 IO 模型，采用**半同步/半反应堆（Half-Sync/Half-Reactor）**模式。

- **主线程 (Main Reactor)**：
  - 持有 `epoll` 实例。
  - 负责监听监听套接字 (`listen_fd`) 的连接请求 (`EPOLLIN`)。
  - 负责监听已连接套接字 (`conn_fd`) 的数据到达事件。
  - **不进行**任何复杂的业务逻辑处理或耗时计算，确保对网络事件的极速响应。
- **任务队列 (Task Queue)**：
  - 主线程收到数据后，将 `(socket_fd, data)` 封装成任务对象，推入任务队列。
  - 使用互斥锁 (`pthread_mutex`) 和条件变量 (`pthread_cond`) 保证线程安全。
- **工作线程池 (Worker Thread Pool)**：
  - 维护一组预创建的线程（如 4-8 个）。
  - 从任务队列中抢占任务。
  - 执行具体的业务逻辑（如：解析协议、查找用户、读写数据库/文件）。
  - 将处理结果写回 Socket。

### 2.2 模块划分

| 模块名称            | 功能描述                             | 涉及关键技术 (对应评分标准)                                  |
| ------------------- | ------------------------------------ | ------------------------------------------------------------ |
| **Network Core**    | 封装 Socket API，管理 Epoll 生命周期 | `socket`, `bind`, `listen`, `epoll_create`, `epoll_wait` (项16) |
| **ThreadPool**      | 管理工作线程，处理并发任务           | `pthread_create`, `mutex`, `cond` (项2, 7, 8, 9)             |
| **Connection Mgr**  | 管理在线用户列表，处理心跳检测       | `std::map`, `time_t` (项12, 13)                              |
| **File Transfer**   | 处理文件请求，执行零拷贝发送         | `stat`, `open`, `sendfile` (项15)                            |
| **Protocol Parser** | 处理 TCP 粘包/拆包，序列化消息       | 指针操作, 缓冲区管理                                         |
| **Client UI**       | 客户端终端图形界面绘制               | `ncurses` 库 (项11)                                          |

## 3. 核心数据结构与协议设计 (Data Structures & Protocols)

### 3.1 应用层通信协议

为了解决 TCP 流式传输中的**粘包**和**半包**问题，自定义定长包头+变长包体的二进制协议。

```
// 消息类型定义
enum MsgType {
    MSG_LOGIN       = 0x01, // 登录
    MSG_CHAT_PUBLIC = 0x02, // 群聊
    MSG_CHAT_PRIVATE= 0x03, // 私聊
    MSG_FILE_REQ    = 0x04, // 文件下载请求
    MSG_FILE_DATA   = 0x05, // 文件数据流
    MSG_HEARTBEAT   = 0x06  // 心跳包
};

// 协议头部 (定长 12 字节)
struct PacketHeader {
    int32_t total_len;  // 数据包总长度 (Header + Body)
    int32_t msg_type;   // 消息类型
    int32_t crc32;      // 校验码 (可选)
};

// 示例：文件请求体
struct FileReqBody {
    char filename[256];
};
```

### 3.2 任务对象 (Task)

用于在 Reactor 和 ThreadPool 之间传递上下文。

```
struct Task {
    int socket_fd;          // 客户端套接字
    void* data;             // 接收到的原始数据包
    size_t data_len;        // 数据长度
    // 函数指针，支持不同的回调逻辑
    void (*task_callback)(int, void*, size_t); 
};
```

### 3.3 在线用户表

使用线程安全的 Map 管理。

```
struct UserClient {
    int socket_fd;
    std::string username;
    time_t last_heartbeat; // 最后一次心跳时间
};

// 全局互斥锁保护
pthread_mutex_t g_user_map_lock;
std::map<int, UserClient*> g_online_users;
```

## 4. 关键技术实现细节 (Key Implementation Details)

### 4.1 Epoll 事件循环 (Main Loop)

采用 **Epoll Level Triggered (LT)** 模式（也可选 ET），兼顾性能与开发难度。

**流程：**

1. `epoll_create()` 创建实例。
2. `epoll_ctl()` 注册 `listen_fd`。
3. `while(true)` 循环调用 `epoll_wait()`。
4. 如果是 `listen_fd` 事件 -> `accept()` -> `epoll_ctl(ADD)` 新连接。
5. 如果是 `client_fd` 事件 -> 读取数据到缓冲区 -> 封装成 `Task` -> `ThreadPool::AddTask()`。

### 4.2 线程池与同步机制

**设计亮点：** 避免了每来一个请求就创建销毁线程的开销。

- **生产者**：主线程 Epoll 循环。
- **消费者**：工作线程。
- **同步**：
  - `pthread_mutex_lock` 保护任务队列。
  - `pthread_cond_wait` 当队列为空时挂起工作线程，节省 CPU。
  - `pthread_cond_signal` 当主线程放入任务时唤醒工作线程。

### 4.3 零拷贝文件传输 (Zero-Copy)

**传统方式**：硬盘 -> 内核缓冲区 -> 用户缓冲区(read) -> 内核套接字缓冲区(write) -> 网卡。 (4次上下文切换，2次CPU拷贝)。 **本项目方式 (sendfile)**：硬盘 -> 内核缓冲区 -> 网卡。

**核心代码逻辑：**

```
void handle_file_request(int client_fd, char* filename) {
    int file_fd = open(filename, O_RDONLY);
    struct stat stat_buf;
    fstat(file_fd, &stat_buf);
    
    // 1. 发送文件头信息 (大小，文件名)
    send_file_header(client_fd, stat_buf.st_size);
    
    // 2. 零拷贝发送内容
    off_t offset = 0;
    // 循环发送直到发完，sendfile 在内核中直接将文件描述符内容导向 socket
    while (offset < stat_buf.st_size) {
        sendfile(client_fd, file_fd, &offset, stat_buf.st_size - offset);
    }
    
    close(file_fd);
}
```

### 4.4 心跳检测机制 (Heartbeat)

为了防止“半死连接”（客户端断网但没发 FIN 包）占用服务器资源。

1. **客户端**：每 5 秒发送一个 `MSG_HEARTBEAT` 空包。
2. **服务端**：
   - 收到包更新 `UserClient.last_heartbeat` 为当前时间。
   - 启动一个独立的**检测线程**，每 10 秒遍历一次 `g_online_users`。
   - 如果 `当前时间 - last_heartbeat > 30秒`，判定掉线，`close(fd)` 并从 Epoll 移除。

## 5. 项目目录结构 (Directory Structure)

```
.
├── bin/                 # 编译输出的可执行文件
├── build/               # 中间目标文件
├── include/             # 头文件 (.h)
│   ├── threadpool.h
│   ├── protocol.h
│   ├── reactor.h
│   └── logger.h
├── src/                 # 源代码 (.cpp/.c)
│   ├── main_server.cpp
│   ├── threadpool.cpp
│   ├── network_epoll.cpp
│   └── file_transfer.cpp
├── client/              # 客户端代码
│   ├── main_client.cpp
│   └── ui_ncurses.cpp
├── file_storage/        # 服务器存放供下载文件的目录
├── Makefile             # 自动化构建脚本
└── README.md            # 项目说明书
```

## 6. 开发计划表

1. **阶段一：基础设施 (Day 1-2)**
   - 实现 Makefile。
   - 封装 Socket 基本操作函数 (Error handling)。
   - 实现线程池 `ThreadPool` 类。
2. **阶段二：网络核心 (Day 3-4)**
   - 实现 Epoll 主循环。
   - 打通 "接收数据 -> 放入队列 -> 线程打印数据" 的流程。
3. **阶段三：协议与业务 (Day 5-6)**
   - 定义 `PacketHeader`。
   - 实现登录、群聊转发逻辑。
   - 实现 `UserMap` 管理。
4. **阶段四：高级功能 (Day 7-8)**
   - 实现 `sendfile` 文件下载功能。
   - 实现心跳检测线程。
5. **阶段五：客户端优化 (Day 9)**
   - 集成 `ncurses` 库，美化界面。
   - 测试与除错。
